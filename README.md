## TODO:

1. Установить и запустить программу. Основной файл `viewlab.py`, который запускает сервер по адресу `127.0.0.1:8050`. Установка требует использования виртуального окружения Python. Соответственно, надо создать окружение `python3 -m venv .venv` и установить зависимости `pip install -r req.txt`. Если будете добавлять какие-то сторонние файлы, как файлы настроек IDE, не забывайте добавлять в `.gitignore`.
2. Описать функциональность программы в этом README. Сделать тут разделы с соответствующей информацией: Установка и запуск, Функциональность, О программе.
3. Описать, какие функции можно добавить в проект, что можно улучшить по интерфейсу (к примеру, чтобы все надписи были на русском), что можно улучшить по структуре - какой рефакторинг можно сделать. По возможности внести изменения (к примеру, сделать, чтобы все надписи были на русском).

## О программе:

Оригинальная программа была разработана Кириллом Духанином и Дмитрием Петуховым в качестве лабораторной работы по методам оптимизации (https://github.com/uiqkos/meth-opt-grad-desc).

Используется фреймворк Plotly Dash (https://dash.plotly.com/).

**(viewlab.py)**

Программа представляет собой визуализацию поиска минимума функции на основе фреймворка Dash.
В панели настроек мы можем выбрать функцию для минимизации, метод минимизации, начальную точку,
критерий остановки, выбор шага. Используемые библиотеки: math, time, functools, operator,
typing. Для создания интерфейса и визуализации используются библиотеки:dash, dash_daq, dash.dependencies, plotly.graph_objects.

Класс Settings Содержит настройки приложения, включая доступные стратегии выбора шага и методы минимизации.
Интерфейс состоит из двух основных частей: dcc.Graph используется для отображения графика, панель настроек.

Функция update_graph отвечает за обновление графика и отображение результатов минимизации при изменении параметров:

1. Получает значения всех входных параметров из элементов интерфейса.
2. Находит соответствующие функции минимизации и стратегии шага из настроек.
3. Преобразует введенное пользователем выражение функции в объект sympy.Expr
4. Запускает выбранный метод минимизации с заданными параметрами.
5. Создает 3D поверхность функции и Отображает путь минимизации
6. Возвращает обновленный график и текст с результатами минимизации

Приложение запускается вызовом app.run_server(debug=True), пользователь может взаимодействовать с ним через веб-браузер.

**all**

Реализация методов оптимизации с помощью библиотеки sympy.

1. gradient*descent*: реализует метод градиентного спуска, преобразуя символьное выражение func в функцию с помощью fs.lambdify
   вычисляет якобиан функции func с помощью fs.jacobi,
2. scipy*nelder_mead*: реализует метод Нелдера-Мида, преобразует выражение func в функцию и использует метод scipy_nelder_mead для поиска минимума
3. newton_descent_classic_next_point: Реализует метод Ньютона с использованием классического метода поиска следующей точки.
   вычисляет якобиан и гессиан функции и использует метод newton_descend для выполнения спуска.
4. newton_descent_linalg_next_point: Реализует метод, но для поиска следующей точки использует решение линейной системы уравнений,
   остальные шаги аналогичны функции newton_descent_classic_next_point.
5. scipy*newton*: Реализует метод Ньютона с использованием библиотеки scipy, использует функцию scipy_newton_cg для минимизации и выводит путь минизации
6. pylbfgs*lbfgs*: Реализует метод L-BFGS, Использует функцию pylbfgs_lbfgs для оптимизации.

**grad_desc**

Функции для настройки скорости обучения:
constant_learning_rate(start_learning_rate: float): Возвращает функцию, которая всегда возвращает фиксированную скорость обучения.

exponential_learning_rate_scheduling(start_learning_rate: float, \_lambda: float):
Реализует экспоненциальное уменьшение скорости обучения. С каждым шагом скорость обучения уменьшается экспоненциально в зависимости от номера эпохи.

polynomial_learning_rate_scheduling(start_learning_rate: float, alpha: float, beta: float):
Полиномиальная функция для изменения скорости обучения. Скорость обучения уменьшается в зависимости от полинома, где alpha и beta задают форму кривой.

piecewise_learning_rate_scheduling(borders: np.array, values: np.array):
Реализует кусочно-заданную функцию для скорости обучения, где на различных интервалах, определяемых borders, используется фиксированное значение
скорости обучения, определяемое values.

constant_rate(lr: float) -> LearningRateFunction:
Еще одна реализация функции с постоянной скоростью обучения.

Методы для поиска оптимальной скорости обучения:
dichotomy_method(stop_delta: float):
Реализует метод дихотомии - делит отрезок пополам и выбирает тот, где значение функции меньше.

golden_ratio_method(stop_delta: float):
Метод золотого сечения - сокращает отрезок поиска, используя пропорции золотого сечения.

Алгоритмы оптимизации:
gradient_descend:
Реализует стандартный градиентный спуск. На каждой итерации функция вычисляет градиент в текущей точке,
обновляет текущую точку с учетом скорости обучения и продолжает до выполнения одного из условий остановки.

gradient_and_newton:
Комбинирует градиентный спуск с методом Ньютона. Сначала выполняется градиентный спуск,
затем переход к методу Ньютона для ускоренного нахождения минимума.

stochastic_gradient_descent:
Реализует стохастический градиентный спуск, где обновления параметров происходят на основе
случайно выбранных мини-батчей данных.

Основные аспекты и условия остановки:
Условия остановки:

1. Достигнуто максимальное количество итераций (max_iter).
2. Малое изменение функции (stop_function_delta).
3. Малое изменение точки (stop_point_delta).
4. Достигнута минимальная норма градиента (stop_grad_norm для SGD).

**nelder**

Оптимизация функции с использованием метода Нелдера-Мида, с небольшой модификацией стандартной функции scipy.optimize.minimize.
Цели: модифицировать стандартную функцию minimize из библиотеки SciPy, чтобы сохранять промежуточные симплексы
при выполнении метода Нелдера-Мида.

**newton**

Релизация различных методов оптимизации функций с использованием методов Ньютона и L-BFGS, а также обеспечивает
возможность гибкого вычисления следующей точки на основе градиента и гессиана функции.

Функция calculate_next_point_classic: Эта функция вычисляет следующую точку оптимизации с использованием классического
подхода метода Ньютона, градиент умножается на обратную матрицу гессиана, чтобы получить направление поиска, затем это
направление масштабируется с использованием функции изменения шага lr (learning rate function) и вычитается из текущей точки last_point, чтобы получить новую точку

Функция calculate_next_point_linear_system: Эта функция также вычисляет следующую точку, но вместо обратной матрицы используется решение линейной системы уравнений
(метод np.linalg.solve), Полученный вектор delta_x (решение системы) представляет направление поиска, и вычитание его из текущей точки дает новую точку.

Функция newton_descend: Эта функция реализует метод Ньютона для оптимизации func. Используется функция calculate_next_point_func для вычисления следующей точки на
каждой итерации, можно задать критерии остановки: stop_function_delta (разница в значениях функции) и stop_point_delta (разница между точками).

Функция scipy_newton_cg: выполнения оптимизации с помощью метода Ньютона с сопряженными градиентами (Newton-CG). Результат включает в себя путь точек, количество итераций, успех и причину остановки.

Функция pylbfgs_lbfgs: Этот метод использует библиотеку lbfgs для выполнения оптимизации с помощью метода L-BFGS, функция возвращает результат оптимизации, включая конечное значение, путь точек и количество итераций.

**funcs**

функции для работы с символьными выражениями в библиотеке SymPy и преобразует их в численные функции, которые могут быть использованы в численных алгоритмах оптимизации, таких как вычисление градиентов и гессианов.
hessian(f: sp.Expr) -> Callable[[Point], np.ndarray] - Эта функция создает численную функцию для вычисления матрицы Гессиана для символьной функции f.

lambdify(f: sp.Expr) -> Callable[[Point], float] - создает численную функцию для вычисления значения символьного выражения f в конкретной точке x.

jacobi(f: sp.Expr) -> Callable[[Point], np.ndarray] - функция создает численную функцию для вычисления Якобиана символьной функции f.

from_sympy_to_plotable_func(f) - преобразует символьное выражение f в функцию, которая используется для построения графиков. Функция tupled, определенная в utils, преобразует результат в кортеж, который удобно использовать для построения графиков.

**utils**

Реализация функций для визуализации математических функций и их производных в двумерном и трёхмерном пространстве.

1. tupled(f)
   Функция-обёртка, которая преобразует аргументы из формы кортежа в отдельные параметры для функции f.
2. derivative(f, vars)
   Вычисляет производные функции f по переменным vars, возвращает список функций, каждая из которых представляет собой производную по одной из переменных.
3. coefs_from_dict(coefs)
   Извлекает коэффициенты для квадратичной функции из словаря coefs.
4. gen_func(coefs)
   Создаёт функцию на основе переданных коэффициентов, которая вычисляет значение квадратичной функции от двух переменных.
5. R2_derivatives(coefs)
   Возвращает функции производных по каждой из переменных для квадратичной функции, заданной коэффициентами coefs.
6. plot_func(f, path=(), limit=10, label='')
   Строит 3D-график функции f с использованием библиотеки Plotly. Если задан path, добавляет его на график, что полезно для визуализации траектории оптимизации.
7. call_counter(f)
   Функция-обёртка, которая считает количество вызовов функции f. Полезно для мониторинга производительности и анализа вызовов.
8. plot_func_with_path_matplotlib(f, path, limit=10)
   Строит 3D-график функции f с использованием Matplotlib и добавляет на график траекторию оптимизации path.
9. plot_2d_with_color(f, path=(), limit=10)
   Строит 2D-график функции f с цветовой картой для значений функции и добавляет траекторию оптимизации.
10. get_color(index, total_triangles) и create_gradient_colormap(n_colors)
    Создают градиентные цветовые схемы, которые используются для визуализации.
11. plot_2d_and_3d_side_by_side(f, path, limit=10, title='', save=False, issimplex=False, text='')
    Создаёт комбинированный график, показывающий функцию в 2D и 3D, а также текстовый блок для дополнительной информации. Может использоваться для сохранения результатов визуализации в файл.

**typing_local**

Набор определений типов и классов, которые используются для реализации и управления различными методами оптимизации.
Типа данных, причины остановки, классы результатов оптимизации, определение функциональных типов.

## Предложения по рефакторингу:

разделить функцию update_graph в файле viewlab.py на несколько.

## Предложения по функционалу

сделать возможность сохранения
